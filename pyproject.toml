[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "vllm-mlx"
version = "0.2.5"
description = "vLLM-like inference for Apple Silicon - GPU-accelerated Text, Image, Video & Audio on Mac"
readme = "README.md"
license = {text = "Apache-2.0"}
requires-python = ">=3.10"
authors = [
    {name = "vllm-mlx contributors"}
]
keywords = ["llm", "mlx", "apple-silicon", "vllm", "inference", "transformers"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: Apache Software License",
    "Operating System :: MacOS",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]

dependencies = [
    "mlx>=0.29.0",
    "mlx-lm>=0.30.2",  # GLM-4 models require 0.30.2+
    "mlx-vlm>=0.1.0",  # VLM support
    "transformers>=4.40.0,<=5.0.0rc1",  # Pinned to avoid rc3 video processing bug
    "tokenizers>=0.19.0",
    "huggingface-hub>=0.23.0",
    "numpy>=1.24.0",
    "pillow>=10.0.0",
    "tqdm>=4.66.0",
    "pyyaml>=6.0",
    "gradio>=4.0.0",
    "requests>=2.28.0",
    "tabulate>=0.9.0",
    # Video processing for VLM
    "opencv-python>=4.8.0",
    # Vision processor (required for transformers AutoProcessor)
    "torchvision>=0.18.0",
    # Resource monitoring
    "psutil>=5.9.0",
    # Server
    "fastapi>=0.100.0",
    "uvicorn>=0.23.0",
    # MCP (Model Context Protocol) support
    "mcp>=1.0.0",
    # JSON Schema validation for structured output
    "jsonschema>=4.0.0",
    # pytz is required by gradio but not declared as a dependency
    # See: https://github.com/waybarrios/vllm-mlx/issues/23
    "pytz>=2024.1",
    # Note: mlx-audio moved to optional [audio] deps due to mlx-lm version conflict
    # See: https://github.com/waybarrios/vllm-mlx/issues/19
    "mlx-embeddings>=0.0.5"
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
    "mypy>=1.0.0",
]
vllm = [
    "vllm>=0.4.0",
]
vision = [
    "torch>=2.3.0",
    "torchvision>=0.18.0",
]
# Audio dependencies for TTS/STT (mlx-audio)
audio = [
    "mlx-audio>=0.2.9",
    "sounddevice>=0.4.0",
    "soundfile>=0.12.0",
    "scipy>=1.10.0",
    "numba>=0.57.0",
    "tiktoken>=0.5.0",
    "misaki[zh,ja]>=0.5.0",  # Chinese (zh) and Japanese (ja) support
    "spacy>=3.7.0",
    "num2words>=0.5.0",
    "loguru>=0.7.0",
    "phonemizer>=3.2.0",
    # Additional multilingual dependencies
    "ordered_set>=4.1.0",    # Required for Chinese TTS
    "cn2an>=0.5.0",          # Chinese number conversion
    "fugashi>=1.3.0",        # Japanese tokenizer
    "unidic-lite>=1.0.0",    # Japanese dictionary for fugashi
    "jieba>=0.42.0",         # Chinese word segmentation
]

[project.urls]
Homepage = "https://github.com/vllm-mlx/vllm-mlx"
Documentation = "https://github.com/vllm-mlx/vllm-mlx#readme"
Repository = "https://github.com/vllm-mlx/vllm-mlx"

[project.entry-points."vllm.platform_plugins"]
mlx = "vllm_mlx.plugin:mlx_platform_plugin"

[project.scripts]
vllm-mlx = "vllm_mlx.cli:main"
vllm-mlx-chat = "vllm_mlx.gradio_app:main"
vllm-mlx-bench = "vllm_mlx.benchmark:main"

[tool.setuptools.packages.find]
where = ["."]
include = ["vllm_mlx*"]

[tool.black]
line-length = 88
target-version = ["py310", "py311", "py312", "py313"]

[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = ["E", "F", "W", "I", "N", "UP", "B", "SIM"]
ignore = ["E501", "B905"]

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
ignore_missing_imports = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
asyncio_mode = "auto"
